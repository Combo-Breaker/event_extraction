{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "from nltk import DependencyGraph\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import pymorphy2\n",
    "from itertools import tee\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import math\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import gensim\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity as cosine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Функции для обработки текста:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## извлечение даты из новости \n",
    "\n",
    "def month_to_num(word):\n",
    "    months = ['января','февраля','марта','апреля','мая','июня','июля','августа','сентября','октября','ноября','декабря']\n",
    "    res = months.index(word)+1\n",
    "    if res < 10:\n",
    "        res = '0'+str(res)\n",
    "    return(str(res))\n",
    "\n",
    "def date(news):\n",
    "    regexp_date = re.compile('Опубликовано\\s[0-9]{1,2}\\s[А-Яа-я]+\\s201\\d') \n",
    "    date_raw = regexp_date.findall(news)[0].lower().split()[1:]\n",
    "    date = str(date_raw[0])+'.'+month_to_num(date_raw[1])+'.'+str(date_raw[2])\n",
    "    return(date)\n",
    "\n",
    "\n",
    "def reverse(date, mode):\n",
    "    if mode == 1:\n",
    "        d,m,y = date.split('.')\n",
    "        return(str(y)+'.'+str(m)+'.'+str(d))\n",
    "    else:\n",
    "        y,m,d = date.split('.')\n",
    "        return(str(d)+'.'+str(m)+'.'+str(y))\n",
    "        \n",
    "           \n",
    "\n",
    "## извлечение текста новости \n",
    "def sents(text):\n",
    "    s = text.split('\\n')\n",
    "    res = [w.split(' .') for w in s if ((len(w) > 1) and w.split()[0] not in ['Опубликовано', 'Авторы:'])]\n",
    "    flat_list = [item for sublist in res for item in sublist]\n",
    "    return(flat_list)\n",
    "\n",
    "\n",
    "\n",
    "## стоп-слова и замена \"компания\" на название компании\n",
    "def clean_lemmas(d):\n",
    "    global cotags\n",
    "    stopwords1 = ['это', 'который', 'год', 'день', 'месяц', 'неделя', 'конец', 'начало', 'рубль', \n",
    "                  'январь','февраль','март','апрель','май','июнь','июль','август','сентябрь','октябрь','ноябрь','декабрь']\n",
    "    words = ['компания_NOUN', 'холдинг_NOUN', 'монополия_NOUN', 'госкомпания_NOUN','нефтекомпания_NOUN',\n",
    "             'энергокомпания_NOUN', 'госкорпорация_NOUN', 'корпорация_NOUN']\n",
    "    morph = pymorphy2.MorphAnalyzer()\n",
    "    d1 = {}\n",
    "    for v in d:\n",
    "        for group in d[v]:\n",
    "            lemmas = set()\n",
    "            for w in d[v][group]:\n",
    "                p = morph.parse(w)[0]\n",
    "                if not 'LATN' in p.tag:\n",
    "                    if w.isalpha() and not (p.normal_form in stopwords.words('russian') or p.normal_form in stopwords1):\n",
    "                        lemmas.add(p.normal_form+'_'+cotags[p.tag.POS])\n",
    "                else:\n",
    "                    lemmas.add(w+'_S')\n",
    "            \n",
    "                    \n",
    "            for w in words:\n",
    "                if w in lemmas:\n",
    "                    lemmas.remove(w)\n",
    "                    lemmas.add(company)\n",
    "            d[v][group] = lemmas\n",
    "        \n",
    "        p = morph.parse(v)[0] \n",
    "        d1[p.normal_form+'_'+cotags[p.tag.POS]] = d[v]      \n",
    "    \n",
    "    \n",
    "    return d1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# разрешение кореференции: замена слов \"компания\" и аналогичных на потенциальные именованные сущности\n",
    "\n",
    "def possible_entities(all_news):\n",
    "    morph = pymorphy2.MorphAnalyzer()\n",
    "    candidates = {}\n",
    "    for news in all_news:\n",
    "        sentences = sents(news)\n",
    "        for s in sentences:\n",
    "            for w in s.split()[1:]:\n",
    "                if w[0].isupper() and len(w) >= 3:\n",
    "                    n = morph.parse(w)[0].normal_form\n",
    "                    if n in candidates:\n",
    "                        candidates[n] += 1\n",
    "                    else:\n",
    "                        candidates[n] = 1\n",
    "\n",
    "    \n",
    "    return(candidates)\n",
    "\n",
    "\n",
    "\n",
    "def replace_with_entity(news_list):\n",
    "    morph = pymorphy2.MorphAnalyzer()\n",
    "    global named_entities\n",
    "    global freq_treshold\n",
    "    global stopwords\n",
    "    news1 = []\n",
    "    named_entity = ''\n",
    "    for sent in news_list:\n",
    "        if len(sent):\n",
    "            res = ''\n",
    "            for w in sent.split():\n",
    "                n = morph.parse(w)[0]\n",
    "                if n.normal_form.lower() in named_entities and named_entities[n.normal_form.lower()] > freq_treshold:\n",
    "                    named_entity = w\n",
    "                    res += ' ' + w\n",
    "                elif n.normal_form in stopwords and named_entity != '':\n",
    "                    try:\n",
    "                        form = morph.parse(named_entity)[0].inflect({n.tag.case}).word\n",
    "                        res += ' ' + w + ' ' +form\n",
    "                    except:\n",
    "                        res += ' ' + w\n",
    "                else:\n",
    "                    res += ' ' + w\n",
    "            \n",
    "            news1.append(res)\n",
    "            \n",
    "    return(news1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Функции для извлечения SVO-троек:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(triples0):\n",
    "    sov = {}  \n",
    "    triples = list(triples0)\n",
    "    for triple in triples:\n",
    "        if triple:\n",
    "            if triple[0][1] == 'VERB':\n",
    "                sov[triple[0][0]] = {'verb': set(), 'obj': set(), 'subj': set()}\n",
    "            if (triple[1] == 'xcomp') and triple[0][0] in sov:\n",
    "                sov[triple[0][0]]['verb'].add(triple[2][0])\n",
    "                         \n",
    "    \n",
    "    for triple in triples:\n",
    "        if triple:\n",
    "            if (triple[1] == 'xcomp') and triple[0][0] in sov:\n",
    "                sov[triple[0][0]]['verb'].add(triple[2][0])\n",
    "            if triple[1] == 'neg' and triple[0][0] in sov:\n",
    "                sov[triple[0][0]]['verb'].add('не') \n",
    "            if triple[1] == 'nsubj' and triple[0][0] in sov:\n",
    "                sov[triple[0][0]]['subj'].add(triple[2][0])\n",
    "            if triple[1] == 'dobj' and triple[0][0] in sov:\n",
    "                sov[triple[0][0]]['obj'].add(triple[2][0])   \n",
    "\n",
    "    for verb in sov:\n",
    "        subj = sov[verb]['subj']\n",
    "        subj_group = set()\n",
    "        added = 1\n",
    "        while (added):\n",
    "            added = 0\n",
    "            for triple in triples:\n",
    "                if (triple[1] == 'dobj' or triple[1] == 'nmod') and triple[0][0] in subj and triple[2][0] not in subj_group:\n",
    "                    subj_group.add(triple[2][0])\n",
    "                    triples.remove(triple)\n",
    "                    added = 1\n",
    "                if (triple[1] == 'appos' or triple[1] == 'name') and triple[0][0] in subj:\n",
    "                    subj_group.add(triple[2][0])\n",
    "                    added = 1\n",
    "                    triples.remove(triple)\n",
    "        sov[verb]['subj'].update(subj_group)\n",
    "                    \n",
    "          \n",
    "    for verb in sov:\n",
    "        obj = sov[verb]['obj']\n",
    "        obj_group = set()\n",
    "        added = 1\n",
    "        while (added):\n",
    "            added = 0\n",
    "            for triple in triples:\n",
    "                if (triple[1] == 'dobj' or triple[1] == 'nmod') and (triple[0][0] in subj or triple[0][0] == verb) and triple[2][0] not in obj_group:\n",
    "                    triples.remove(triple)\n",
    "                    obj_group.add(triple[2][0])\n",
    "                    added = 1\n",
    "        sov[verb]['obj'].update(obj_group)\n",
    "    \n",
    "           \n",
    "    sov1 = {}\n",
    "    for v in sov:\n",
    "        if not 'del' in sov[v]['subj']:\n",
    "            if not 'не' in sov[v]['verb']:\n",
    "                sov1[v] = sov[v]\n",
    "            else:\n",
    "                sov[v]['verb'].remove('не')\n",
    "                sov1['не '+v] = sov[v]\n",
    "    \n",
    "    return clean_lemmas(sov1)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def triples_extraction(path):\n",
    "    processed_sentences = []\n",
    "    sentence = []\n",
    "\n",
    "    for line in codecs.open(path, 'r', 'utf-8'):\n",
    "        if len(line) == 1:\n",
    "            processed_sentences.append(sentence)\n",
    "            sentence = []\n",
    "        else:\n",
    "            word = line.split(\"\\t\")\n",
    "            sentence.append(word)\n",
    "\n",
    "    deps = []\n",
    "    for sentence in processed_sentences:\n",
    "        s = u\"\"\n",
    "        for line in sentence:\n",
    "            s += u\"\\t\".join(line) + u'\\n'\n",
    "        deps.append(s)\n",
    "    \n",
    "    triples = []\n",
    "    for sent_dep in deps:\n",
    "        try:\n",
    "            graph = DependencyGraph(tree_str=sent_dep)\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            res = extract(graph.triples())\n",
    "            triples.append(res)\n",
    "        except:\n",
    "            pass\n",
    "    tr = [d for d in triples if sum([len(v) for v in d.values()])>= 3]\n",
    "    return(triples)\n",
    "\n",
    "def sov_by_date(texts_by_date):\n",
    "    sov = {}\n",
    "    for d in tqdm(texts_by_date):\n",
    "        with open ('sents.txt', 'w') as f:\n",
    "            for i in texts_by_date[d]:\n",
    "                if len(i) > 1:\n",
    "                    for j in i:\n",
    "                        f.write(j)\n",
    "                        f.write('\\n')\n",
    "        ! cat sents.txt | docker run -a stdout > log.txt --rm -i inemo/syntaxnet_rus > data.conll\n",
    "        sov[d] = triples_extraction('data.conll')\n",
    "        \n",
    "    for d in sov:\n",
    "        for tr in sov[d]:\n",
    "            for v1 in tr:\n",
    "                for cat in tr[v1]:\n",
    "                    tr[v1][cat] = list(tr[v1][cat])\n",
    "    return sov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Извлечение троек:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# для упорядоченных по компаниям новостей:\n",
    "# Нужно ввести название компании (на русском языке), тексты новостей по умолчанию лежат в папке utilities.\n",
    "company = input('Название компании: ')\n",
    "\n",
    "companies = {'Алроса': 'alrosa',\n",
    "             'Газпром': 'gazprom',\n",
    "             'Интер РАО': 'interrao',\n",
    "             'Лукойл': 'lukoil',\n",
    "             'Норникель': 'nornikel',\n",
    "             'Новатэк': 'novatek',\n",
    "             'Росатом': 'rosatom',\n",
    "             'Русснефть': 'russneft',\n",
    "             'Татнефть': 'tatneft',\n",
    "             'Транснефть': 'transeft',\n",
    "             'test':'test'\n",
    "            }\n",
    "\n",
    "\n",
    "with open('utilities/%s.txt' % companies[company]) as f:\n",
    "        raw = f.read()\n",
    "        \n",
    "raw = raw.replace('.', ' .').replace(',', ' ,')\n",
    "raw = re.sub(r'[«»\"]','', raw)\n",
    "news = raw.split('--------------------------------------------------------------------------')\n",
    "\n",
    "news = list(set(news))\n",
    "print('Новостей: ', len(news))\n",
    "\n",
    "\n",
    "texts_by_date = {}\n",
    "for n in news:\n",
    "    try:\n",
    "        d = date(n)\n",
    "    except:\n",
    "        pass\n",
    "    s = sents(n)\n",
    "    if d in texts_by_date:\n",
    "        texts_by_date[d].append(s)\n",
    "    else:\n",
    "        texts_by_date[d]=[s] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# для неупорядоченных по компаниям новостей\n",
    "        \n",
    "with open('path_to_txt') as f:\n",
    "        raw = f.read()\n",
    "        \n",
    "        \n",
    "        \n",
    "raw = raw.replace('.', ' .').replace(',', ' ,')\n",
    "raw = re.sub(r'[«»\"]','', raw)\n",
    "news = raw.split('--------------------------------------------------------------------------')\n",
    "\n",
    "news = list(set(news))\n",
    "print('Новостей: ', len(news))\n",
    "\n",
    "freq_treshold = 100 # пороговое значение: если слово, начинающееся с заглавной буквы, встретилось в тексте больше freq_treshold раз,\n",
    "                    # то оно считается кандидатом в \"именованную сущностью\"\n",
    "\n",
    "# этот список нужно расширить :(\n",
    "stopwords = ['компания', 'холдинг', 'монополия', 'госкомпания','нефтекомпания','энергокомпания', 'госкорпорация', 'корпорация']\n",
    "named_entities = possible_entities(news)\n",
    "\n",
    "texts_by_date = {}\n",
    "for n in news:\n",
    "    try:\n",
    "        d = date(n)\n",
    "    except:\n",
    "        pass\n",
    "    s = replace_with_entity(sents(n))\n",
    "    if d in texts_by_date:\n",
    "        texts_by_date[d].append(s)\n",
    "    else:\n",
    "        texts_by_date[d]=[s]       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# извлечение SVO-троек:\n",
    "\n",
    "data = sov_by_date(texts_by_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Построение эмбеддингов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2v_tag(lemma):    \n",
    "    cotags = {'ADJF':'A', # pymorphy2: rusvectores\n",
    "                'ADJS' : 'A', \n",
    "                'ADVB' : 'ADV', \n",
    "                'COMP' : 'A',\n",
    "                'CONJ' : 'CONJ',\n",
    "                'GRND' : 'V',\n",
    "                'NUMR' : 'NUM',\n",
    "                'INFN' : 'V', \n",
    "                'NOUN' : 'S', \n",
    "                'PRED' : 'V', \n",
    "                'PRTF' : 'A', \n",
    "                'PRTS' : 'V', \n",
    "                'VERB' : 'V',\n",
    "                'NPRO' : 'V',\n",
    "                'PRCL' : 'PART',\n",
    "                'PREP': 'PR'} \n",
    "    word, tag = lemma.split('_')\n",
    "    return (word + '_' + cotags[tag])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## функции для вычисления tf-idf по всему корпусу текстов\n",
    "\n",
    "\n",
    "\n",
    "def compute_tfidf(corpus):\n",
    "    def compute_tf(text):\n",
    "        tf_text = Counter(text)\n",
    "        for i in tf_text:\n",
    "            tf_text[i] = tf_text[i]/float(len(text))\n",
    "        return tf_text\n",
    "    def compute_idf(word, corpus):\n",
    "        return math.log10(len(corpus)/sum([1.0 for i in corpus if word in i]))\n",
    "\n",
    "    documents_list = []\n",
    "    for text in corpus:\n",
    "        tf_idf_dictionary = {}\n",
    "        computed_tf = compute_tf(text)\n",
    "        for word in computed_tf:\n",
    "            tf_idf_dictionary[word] = computed_tf[word] * compute_idf(word, corpus)\n",
    "        documents_list.append(tf_idf_dictionary)\n",
    "    return documents_list\n",
    "\n",
    "def sents_by_words(text):\n",
    "    def date(news):\n",
    "        try:\n",
    "            regexp_date = re.compile('Опубликовано\\s[0-9]{1,2}\\s[А-Яа-я]+\\s201\\d') \n",
    "            date_raw = regexp_date.findall(news)[0].lower().split()[1:]\n",
    "            date = str(date_raw[0])+'.'+month_to_num(date_raw[1])+'.'+str(date_raw[2])\n",
    "            return(date)\n",
    "        except:\n",
    "            return '00.00.0000'\n",
    "\n",
    "    s = text.split('\\n')\n",
    "    res = [w.split(' .') for w in s if ((len(w) > 1)) and w.split()[0]]\n",
    "    flat_list = [item for sublist in res for item in sublist]\n",
    "    l = 'Дата:'+str(date(text)) +' ' + ' '.join([s for s in flat_list])\n",
    "    words = ' '.join([morph.parse(s)[0].normal_form for s in l.split()]).split()\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## вычисляем tf-idf для всего корпуса сразу: соответствующие веса хранятся в словаре по датам\n",
    "\n",
    "corpus = []\n",
    "for n in news:\n",
    "    corpus.append(sents_by_words(n))\n",
    "    \n",
    "tf_idf = compute_tfidf(corpus)\n",
    "\n",
    "tf_idf_by_date = {}\n",
    "\n",
    "for n in tf_idf:\n",
    "    for key in n:\n",
    "        if key.startswith('дата:'):\n",
    "            d = key[5:]\n",
    "    if d not in tf_idf_by_date:\n",
    "        tf_idf_by_date[d] = n\n",
    "    else:\n",
    "        s = dict(Counter(tf_idf_by_date[d])+Counter(n))\n",
    "        tf_idf_by_date[d] = s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## предобученная модель Rusvectores\n",
    "\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(\"news_mystem_skipgram_1000_20_2015.bin.gz\", binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## перевод SVO-троек в эмбеддинги событий\n",
    "\n",
    "triples_vec = {}\n",
    "\n",
    "\n",
    "for date in data:\n",
    "    tf_idf = tf_idf_by_date[date]\n",
    "    embedding = []\n",
    "    for tr in data[date]:       \n",
    "        for verb in tr:\n",
    "            verb_vec = np.zeros(1000)\n",
    "            if tr[verb]['verb'] == []:\n",
    "                try:\n",
    "                    for v in verb.split():\n",
    "                        try:\n",
    "                            tfidf = tf_idf[v.split('_')[0]]\n",
    "                        except:\n",
    "                            tfidf = 0.1\n",
    "                        verb_vec += model[w2v_tag(v)]*tfidf\n",
    "                except:\n",
    "                    pass\n",
    "            else:\n",
    "                try:\n",
    "                    try:\n",
    "                        tfidf1 = tf_idf[verb.split('_')[0]]     \n",
    "                    except:\n",
    "                        tfidf1 = 0.1\n",
    "                    try:\n",
    "                        tfidf2 = tf_idf[tr[verb]['verb'].split('_')[0]]\n",
    "                    except:\n",
    "                        tfidf2 = 0.1\n",
    "        \n",
    "                    verb_vec = model[w2v_tag(verb)]*tfidf1 + model[w2v_tag(tr[verb]['verb'])]*tfidf2\n",
    "                except:\n",
    "                    try:\n",
    "                        try:\n",
    "                            tfidf = tf_idf[tr[verb]['verb'].split('_')[0]]     \n",
    "                        except:\n",
    "                            tfidf = 0.1\n",
    "                            \n",
    "                        verb_vec = model[w2v_tag(tr[verb]['verb'])] * tfidf\n",
    "                        for v in verb.split():\n",
    "                            try:\n",
    "                                tfidf = tf_idf[v.split('_')[0]] \n",
    "                            except:\n",
    "                                tfidf = 0.1\n",
    "                            verb_vec += model[w2v_tag(v)] * tfidf\n",
    "                    except:\n",
    "                        pass\n",
    "            \n",
    "            \n",
    "            obj_vec = np.zeros(1000)\n",
    "            for w in tr[verb]['obj']:\n",
    "                if w2v_tag(w) in model:\n",
    "                    try:\n",
    "                        tfidf = tf_idf[w.split('_')[0]]\n",
    "                    except:\n",
    "                        tfidf = 0.1    \n",
    "                    obj_vec += model[w2v_tag(w)] * tfidf\n",
    "                \n",
    "            subj_vec = np.zeros(1000)\n",
    "            for w in tr[verb]['subj']:\n",
    "                if w2v_tag(w) in model:\n",
    "                    try:\n",
    "                        tfidf = tf_idf[w.split('_')[0]]     \n",
    "                    except:\n",
    "                        tfidf = 0.1\n",
    "                    subj_vec += model[w2v_tag(w)] * tfidf\n",
    "            \n",
    "            average_emb = (verb_vec + subj_vec + obj_vec) / 3\n",
    "            if reverse(date,1) not in triples_vec:\n",
    "                triples_vec[reverse(date,1)] = [average_emb]\n",
    "            else:\n",
    "                triples_vec[reverse(date,1)].append(average_emb)\n",
    "\n",
    "                \n",
    "for date in triples_vec:\n",
    "    if len(triples_vec[date]) != 1:\n",
    "        triples_vec[date] = np.array(triples_vec[date]).mean(axis=0)\n",
    "    else:\n",
    "        triples_vec[date] = triples_vec[date][0]\n",
    "\n",
    "for date in triples_vec:\n",
    "    if len(triples_vec[date]) == 1:\n",
    "        triples_vec[date] = triples_vec[date][0]\n",
    "                            \n",
    "            \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict([triples_vec]).transpose().sort_index()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = ['2017.05.25','2017.05.15','2017.03.29','2017.01.28']\n",
    "\n",
    "for d1 in d:\n",
    "    for d2 in d:\n",
    "        c = cosine(triples_vec[d1].reshape(1,-1), triples_vec[d2].reshape(1,-1))[0]\n",
    "        print(d1, ' ', d2, ' ', c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
